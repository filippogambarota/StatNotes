[
  {
    "path": "posts/brms_notes/",
    "title": "BRMS Notes",
    "description": {},
    "author": [
      {
        "name": "Filippo Gambarota",
        "url": {}
      }
    ],
    "date": "2022-01-05",
    "categories": [],
    "contents": "\n\nContents\nThe model\nPredict Function\nFitted\n\nThe model\n\n\nfit <- brm(mpg ~ disp + vs, data = mtcars)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.092 seconds (Warm-up)\nChain 1:                0.02 seconds (Sampling)\nChain 1:                0.112 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.058 seconds (Warm-up)\nChain 2:                0.019 seconds (Sampling)\nChain 2:                0.077 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 6e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.091 seconds (Warm-up)\nChain 3:                0.02 seconds (Sampling)\nChain 3:                0.111 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.068 seconds (Warm-up)\nChain 4:                0.02 seconds (Sampling)\nChain 4:                0.088 seconds (Total)\nChain 4: \n\nPredict Function\nE’ importante mettere il seed perchè anche senza newdata i risultati sono leggermente diversi.\nla funzione predict() di default mette re_formula = NULL ovvero considera anche i group-level effects (effetti random). In questo modello non ci sono effetti random quindi re_formula = NULL e re_formula = NA (cosi si sopprimono gli effetti random) sono la stessa cosa.\n\n\nset.seed(22)\n\npredict(fit, re_formula = NULL) %>% # Random effect considered\n  tibble()\n\n\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.0           3.66     14.8        29.3\n 2           22.0           3.68     14.8        29.2\n 3           25.5           3.54     18.6        32.6\n 4           19.9           3.64     12.7        27.1\n 5           14.7           3.56      7.55       21.7\n 6           21.0           3.63     14.0        28.1\n 7           14.7           3.48      8.00       21.4\n 8           24.0           3.51     17.2        30.7\n 9           24.3           3.52     17.5        31.4\n10           23.3           3.54     16.7        30.3\n# … with 22 more rows\n\nset.seed(22)\n\npredict(fit, re_formula = NA) %>% # Random effect excluded\n  tibble()\n\n\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.0           3.66     14.8        29.3\n 2           22.0           3.68     14.8        29.2\n 3           25.5           3.54     18.6        32.6\n 4           19.9           3.64     12.7        27.1\n 5           14.7           3.56      7.55       21.7\n 6           21.0           3.63     14.0        28.1\n 7           14.7           3.48      8.00       21.4\n 8           24.0           3.51     17.2        30.7\n 9           24.3           3.52     17.5        31.4\n10           23.3           3.54     16.7        30.3\n# … with 22 more rows\n\nFitted\nLa funzione fitted() non prende in considerazione la varianza residua e quindi semplicemente riporta il sampling dalla posterior (se l’argomento newdata è missing). Quindi riporta il valore fittato dal modello per ogni osservazione.\n\n\nset.seed(22)\n\nfitted(fit) %>% \n  tibble()\n\n\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.1          1.31       19.4       24.6\n 2           22.1          1.31       19.4       24.6\n 3           25.5          0.940      23.7       27.4\n 4           19.9          1.28       17.4       22.5\n 5           14.7          0.903      12.9       16.4\n 6           21.1          1.13       18.9       23.4\n 7           14.7          0.903      12.9       16.4\n 8           24.0          0.929      22.3       25.9\n 9           24.3          0.926      22.5       26.1\n10           23.3          0.957      21.4       25.2\n# … with 22 more rows\n\npredict(fit) %>% # wider CI given the residual variance\n  tibble()\n\n\n# A tibble: 32 × 1\n   .[,\"Estimate\"] [,\"Est.Error\"] [,\"Q2.5\"] [,\"Q97.5\"]\n            <dbl>          <dbl>     <dbl>      <dbl>\n 1           22.0           3.66     14.8        29.3\n 2           22.0           3.68     14.8        29.2\n 3           25.5           3.54     18.6        32.6\n 4           19.9           3.64     12.7        27.1\n 5           14.7           3.56      7.55       21.7\n 6           21.0           3.63     14.0        28.1\n 7           14.7           3.48      8.00       21.4\n 8           24.0           3.51     17.2        30.7\n 9           24.3           3.52     17.5        31.4\n10           23.3           3.54     16.7        30.3\n# … with 22 more rows\n\nLa funzione posterior_epred() o posterior_linepred() hanno due proprietà distintive:\nnon tengono in considerazione la varianza residua (come fitted())\ntrasformano i valori predetti usando la link function\nPer far equivalere fitted() e posterior_linepred() o posterior_epred() bisogna aggiungere summary = FALSE a fitted() in modo restituisca tutte le iterazioni.\n\n\nset.seed(22)\n\nfitted(fit, summary = FALSE)[1:10]\n\n\n [1] 20.22751 22.77145 22.40708 23.11871 19.90737 22.24290 21.90155\n [8] 22.55775 22.43028 22.71727\n\nposterior_epred(fit)[1:10]\n\n\n [1] 20.22751 22.77145 22.40708 23.11871 19.90737 22.24290 21.90155\n [8] 22.55775 22.43028 22.71727\n\nposterior_linpred(fit)[1:10]\n\n\n [1] 20.22751 22.77145 22.40708 23.11871 19.90737 22.24290 21.90155\n [8] 22.55775 22.43028 22.71727\n\nCi sono altri argomenti della funzione ma l’aspetto principale è considerare o meno la varianza residua. Summary:\nfitted(): estrae dalla posterior i valori considerando solo l’incertezza della media con o senza effetti random (group-level effect; re_formula = NA)\nposterior_epred() e posterior_linpred() fanno la stessa cosa ma restituiscono tutte le iterazioni e trasformano la variabile con la link function\n\npredict(): estrae dalla posterior predictive distribution tenendo in considerazione tutte le fonti di variabilità. Anche predict può considerare o meno i group-level effect usando re_formula =\nposterior_predict() fa la stessa cosa ma restituisce tutte le iterazioni\n\n",
    "preview": {},
    "last_modified": "2022-01-05T01:21:44+01:00",
    "input_file": {}
  },
  {
    "path": "posts/contrast_coding/",
    "title": "Contrast Coding",
    "description": {},
    "author": [
      {
        "name": "Filippo Gambarota",
        "url": {}
      }
    ],
    "date": "2022-01-05",
    "categories": [],
    "contents": "\nConstrast Coding Examples\nDifference between treatment contrast (aka dummy coding) and sum contrast:\ndummy coding: one level is coded as reference (0) and each parameter is the difference between the reference and the selected level. For a two level factor inside a lm we have two parameters the intercept (\\(\\beta_0\\)) that is the mean when the predictior(s) is 0 so the mean of the reference level and \\(\\beta_1\\) that represents the increase of \\(y\\) for a unit increase in \\(x_i\\). A unit increase is the difference between 0 and 1 (two levels) so \\(\\beta_1\\) is the mean difference between level 0 and level 1.\nSome fake data:\n\n\ny <- c(rnorm(20, 10, 5), rnorm(20, 14, 9)) # simulate a response variable y\n\nsim <- data.frame(y, id = 1:40, gruppo = factor(rep(c(\"controllo\", \"sperimentale\"), each = 20))) # create a dataset\n\nsim$gruppo_dummy <- sim$gruppo\nsim$gruppo_sum_1 <- sim$gruppo\nsim$gruppo_sum_2 <- sim$gruppo\n\n# Setting contrasts\n\ncontrasts(sim$gruppo_dummy) <- contr.treatment\ncontrasts(sim$gruppo_sum_1) <- contr.sum\ncontrasts(sim$gruppo_sum_2) <- c(-0.5, 0.5) # sum contrast with 0.5, -0.5\n\n\n\nFitting the models:\n\n\nfit1 <- lm(y ~ gruppo, data = sim)\nfit2 <- lm(y ~ gruppo_sum_1, data = sim)\nfit3 <- lm(y ~ gruppo_sum_2, data = sim)\n\ncoef(fit1)\n\n\n       (Intercept) grupposperimentale \n          8.280279           3.605870 \n\ncoef(fit2)\n\n\n  (Intercept) gruppo_sum_11 \n    10.083214     -1.802935 \n\ncoef(fit3)\n\n\n  (Intercept) gruppo_sum_21 \n     10.08321       3.60587 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-05T00:37:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/lme4_repeated_measures/",
    "title": "Lme4 and Repeated Measures",
    "description": {},
    "author": [
      {
        "name": "Filippo Gambarota",
        "url": {}
      }
    ],
    "date": "2022-01-05",
    "categories": [],
    "contents": "\n\nContents\nAfex and lme4\nSummary\n\n\n\n\nlibrary(lme4)\nlibrary(afex)\n\n\n\n\n\ndat <- readRDS(\"test.rds\")\n\n\n\nAfex and lme4\nAttenzione che quando si carica afex o LmerTest quando si rifitta un modello con lme4 questo viene convertito con il contr.sum solo per la funzione anova(fit).\nSono riuscito ad avere praticamente lo stesso output tra lme4 e afex (in generale aov() function). Ci sono due aspetti fondamentali:\nContrast Coding\nRandom effect specification\nDi default, afex o comunque tutti i pacchetti che eseguono un’ANOVA settano i constrasti come contr.sum. Questo permette di interpretare in modo corretto i main effect ma sopratutto le interazioni.\nRispetto al random effect sono ancora in dubbio. Con la funzione afex::aov_car() dove la parte di errore viene esplicitata (al contrario di afex::aov_ez()), nel caso di within subject variables si deve esplicitamente scrivere Error(group/var1*var2...) questo significa che var1, var2... sono ripetute sui soggetti (group; in generale i soggetti).\nCon lme4 di solito ho sempre messo (1|subject) per gestire la misura ripetuta dei soggetti e ho lasciato il constrast coding di default.\n\n\n# Afex Model\nfit_afex <- fit_car <- afex::aov_car(Test_Memory_Abs ~ Emotion * Group + Error(Subject/Emotion), data = dat)\n\n# Lmer Model\nfit_lmer <- lmer(Test_Memory_Abs ~ Emotion * Group + (1|Subject), data = dat)\n\n\n\nFittando i due modelli in questo modo, i risultati con la funzione anova(modello) effettivamente sono diversi.\n\n\nanova(fit_lmer)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n               Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \nEmotion       142.499  71.250     2 3485.2 69.2109 < 2.2e-16 ***\nGroup          19.106  19.106     1   35.0 18.5595 0.0001268 ***\nEmotion:Group   3.285   1.643     2 3485.2  1.5955 0.2029527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfit_afex\n\n\nAnova Table (Type 3 tests)\n\nResponse: Test_Memory_Abs\n         Effect          df  MSE         F  ges p.value\n1         Group       1, 35 0.13 18.69 *** .248   <.001\n2       Emotion 1.57, 54.92 0.05 55.70 *** .378   <.001\n3 Group:Emotion 1.57, 54.92 0.05      1.30 .014    .276\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\nAnche aggiustando i contrasti come su afex quindi mettendo contr.sum i risultati restano gli stessi per lmer. Immagino che anova(fit) non faccia quindi esattamente il test quando lmerTest non è caricato.\nOra carico lmerTest e rifitto il modello. Mentre i parametri non cambiano, l’F-test è molto simile a quello di afex e quindi immagino che abbia ricodificato i contrasti.\n\n\nlibrary(lmerTest)\n\n# Lmer Model\nfit_lmer <- lmer(Test_Memory_Abs ~ Emotion * Group + (1|Subject), data = dat)\n\nanova(fit_afex)\n\n\nAnova Table (Type 3 tests)\n\nResponse: Test_Memory_Abs\n              num Df den Df      MSE       F     ges    Pr(>F)    \nGroup         1.0000 35.000 0.130147 18.6878 0.24832 0.0001215 ***\nEmotion       1.5691 54.917 0.051118 55.7000 0.37765 2.016e-12 ***\nGroup:Emotion 1.5691 54.917 0.051118  1.2976 0.01394 0.2761300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(fit_lmer)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n               Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \nEmotion       142.499  71.250     2 3485.2 69.2109 < 2.2e-16 ***\nGroup          19.106  19.106     1   35.0 18.5595 0.0001268 ***\nEmotion:Group   3.285   1.643     2 3485.2  1.5955 0.2029527    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCi delle differenze che in parte sono dovute al fatto che lme4 lavora in modo diverso da afex sopratutto perchè i p-value e i df non sono disponibili di default. Tuttavia la parte random dovrebbe contenere anche il within-subject factor emotion. Questo punto mi è leggermente meno chiaro:\nvedi qui\nIn ogni caso l’idea è che se ho dei fattori within-subjects devo aggiungerli nested nel soggetto quindi (1|subjects/var) o (1|subject:var). Queste due scritture sono assolutamente identiche.\n\n\nfit_lmer <- lmer(Test_Memory_Abs ~ Emotion * Group + (1|Subject) + (1|Subject:Emotion), data = dat)\n\nanova(fit_lmer)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n               Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \nEmotion       114.261  57.131     2 69.928 55.7699 3.309e-15 ***\nGroup          19.032  19.032     1 35.013 18.5784  0.000126 ***\nEmotion:Group   2.640   1.320     2 69.928  1.2886  0.282119    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfit_afex\n\n\nAnova Table (Type 3 tests)\n\nResponse: Test_Memory_Abs\n         Effect          df  MSE         F  ges p.value\n1         Group       1, 35 0.13 18.69 *** .248   <.001\n2       Emotion 1.57, 54.92 0.05 55.70 *** .378   <.001\n3 Group:Emotion 1.57, 54.92 0.05      1.30 .014    .276\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\nI valori di\nOra i risultati sono praticamente identici.\nSummary\nlme4 di default non fornisce p-value e quindi anche la funzione anova() non restituisce direttamente il test su ogni effetto.\nDi default R assume il treatment coding che non permette la corretta intepretazione del risultato di anova(fit). In questo caso però comunque lmer non cambia anche cambiando i contrasti.\nquando uso lmerTest i risultati sono simili tra afex e lme4. In questo caso suppongo che lmerTest aggiusti i contrasti per avere lo stesso risultato.\nComunque i singoli parametri del modello non cambiano tra lmer e lmerTest::lmer() a meno che non si cambi esplicitamente il contrast coding.\nQuando ho dei fattori within subjects devo comunque inserirli nella parte random. La scrittura è (1|subject) + (1|subject:var1) e così via per tutte le variabilit within subjects.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-05T01:22:47+01:00",
    "input_file": {}
  },
  {
    "path": "posts/machine_learning/",
    "title": "Machine Learning Notes",
    "description": {},
    "author": [
      {
        "name": "Filippo Gambarota",
        "url": {}
      }
    ],
    "date": "2022-01-05",
    "categories": [],
    "contents": "\n\nContents\nPackages\nFunctions\nLOO-CV\nCustom LOO-CV function\nLOO-CV and model complexity\nLOO-CV and Lasso regression\n\n\nPackages\n\n\nlibrary(caret)\nlibrary(glmnet)\nlibrary(purrr)\n\n\n\nFunctions\n\n\nget_rmse <- function(errors){\n  sqrt(mean(errors^2))\n}\n\n\n\nLOO-CV\nCustom LOO-CV function\nThis function implement a not elegant function for testing the loo-cv. Is useful for understanding the idea:\n\n\nmy_loocv <- function(fit){\n  \n  dat <- fit$model\n  y <- all.vars(fit$call)[1]\n  \n  errors <- vector(mode = \"numeric\", length = nrow(dat))\n  \n  for(i in 1:nrow(dat)){\n    to_pred_i <- dat[i, ]\n    fit_no_i <- update(fit, data = dat[-i, ])\n    pred_i <- predict(fit_no_i, to_pred_i)\n    errors[i] <- pred_i - to_pred_i[1, y]\n  }\n  return(errors)\n}\n\n\n\nComparing with the function from caret, this is the OLS model:\n\n\ncaret_ols <- train(mpg ~ disp + wt, \n               data = mtcars, \n               trControl=trainControl(method=\"LOOCV\"), \n               method=\"lm\")\n\n\n\nMy custom function:\n\n\nstandard_ols <- lm(mpg ~ disp + wt, data = mtcars)\nmy_loocv <- get_rmse(my_loocv(standard_ols))\n\n\n\nThe output is exactly the same:\n\n\ncaret_ols$results[\"RMSE\"] == my_loocv\n\n\n  RMSE\n1 TRUE\n\nLOO-CV and model complexity\n\n\npredictors <- colnames(mtcars) # getting all predictors\npredictors <- predictors[-(predictors == \"mpg\")] # keeping only Xs\n\nfit_list <- vector(mode = \"list\", length = length(predictors))\n\nfit_i <- lm(mpg ~ 1, data = mtcars)\n\nfor(i in 1:length(predictors)){\n  fit_i <- update(fit_i, formula(paste(\". ~ . +\", predictors[i])))\n  fit_list[[i]] <- fit_i\n}\n\n\n\nComputing the actual LOO-CV:\n\n\nmy_loocv <- function(fit){\n  \n  dat <- fit$model\n  y <- all.vars(fit$call)[1]\n  \n  errors <- vector(mode = \"numeric\", length = nrow(dat))\n  \n  for(i in 1:nrow(dat)){\n    to_pred_i <- dat[i, ]\n    fit_no_i <- update(fit, data = dat[-i, ])\n    pred_i <- predict(fit_no_i, to_pred_i)\n    errors[i] <- pred_i - to_pred_i[1, y]\n  }\n  return(errors)\n}\n\ncv <- map_dbl(fit_list, function(x) get_rmse(my_loocv(x))) # get loo-cv mean error\nnpred <- map_dbl(fit_list, function(i) length(all.vars(i$call))-2) # get number of predictors\nr2 <- map_dbl(fit_list, function(mod) summary(mod)$r.squared) # get rsquared from fitted models\n\nloo_cv <- data.frame(\n  cv, r2, npred\n)\n\n# Plotting\n\nloo_cv %>% \n  tidyr::pivot_longer(c(1,2), names_to = \"measure\", values_to = \"value\") %>% \n  ggplot(aes(x = npred, y = value)) +\n  geom_line() +\n  geom_point(size = 3) +\n  facet_wrap(~measure, scales = \"free\") +\n  cowplot::theme_minimal_grid()\n\n\n\n\nLOO-CV and Lasso regression\n\n\ngrid <- 10^seq(1, -2, length = 100) # grid of lambda values\nx <- model.matrix(mpg ~ ., mtcars)[, -1] # predictors\ny <- mtcars$mpg # response variable\n\n\n\nFitting the lasso regression:\n\n\nfit_lasso <- glmnet(x, y, alpha = 1, lambda = grid)\n\n\n\nCustom function for computing the lasso and loo-cv:\n\n\nmy_loocv_lasso <- function(dat, fit){\n  \n  errors <- vector(mode = \"list\", length = nrow(dat))\n  \n  for(i in 1:nrow(dat)){\n    to_pred_i <- x[i, ]\n    fit_no_i <- glmnet(x[-i, ], y[-i], alpha = 1, lambda = grid)\n    pred_i <- predict(fit_no_i, newx = t(to_pred_i))\n    errors[[i]] <- pred_i - y[i]\n  }\n  return(errors)\n} \n\nget_min <- function(target, to_minimize){\n  target[which.min(to_minimize)]\n}\n\n\n\nComputing the loo-cv:\n\n\nerrors_lasso <- my_loocv_lasso(mtcars, fit_lasso) # loo-cv\n\ncv_lasso <- do.call(rbind, errors_lasso) # combining lists\n\nmse_lasso <- apply(cv_lasso, 2, function(x) mean(x^2)) # computing error\n\n\n\nPlotting the \\(\\lambda\\) value as a function of the mean-squared error:\n\n\nplot(grid, mse_lasso)\n\n\n\n\nThe minimum error is associated with the 0.7564633.\n\n\n\n",
    "preview": "posts/machine_learning/distill-preview.png",
    "last_modified": "2022-01-05T01:23:01+01:00",
    "input_file": {}
  },
  {
    "path": "posts/sensitivity_analysis/",
    "title": "Sensitivity Analysis",
    "description": {},
    "author": [
      {
        "name": "Filippo Gambarota",
        "url": {}
      }
    ],
    "date": "2022-01-05",
    "categories": [],
    "contents": "\n\nContents\nGeneral idea\nPower by simulation\nSensitivity Analysis\nScript\nBayesian\nSession info\nReferences\n\n\n\n# Packages\n\nlibrary(tidyverse)\nlibrary(pwr)\nlibrary(BayesFactor)\n\n# Seed for simulation\n\nset.seed(2021)\n\n\n\nGeneral idea\nThe sensitivity analysis is a way to estimate the effect size that a given experiment can reach with a certain sample size, desired power and alpha level [@perugini2018practical].\nThe power analysis is usually considered a procedure that estimate a single number (i.e., the sample size) required for a given statistical analysis to reach a certain power level. Is better to consider the power level as a function with fixed and free parameters.\nIn the case of the a priori power analysis, we fix the power level (e.g., \\(1 - \\beta = 0.80\\)) the alpha level (e.g., \\(\\alpha = 0.05\\)“) and the hypothetical effect size (e.g, \\(d = 0.3\\)). Then we simulate or derive analytically the minimum sample size required for reaching the target power level, given the effect size.\nA more appropriate approach is to consider the sample size a free parameter and calculate the power level for a range of sample size, obtaining the power curve. This is very easy using the pwr package. We assume:\n\\(1-\\beta = 0.8\\)\n\\(\\alpha = 0.05\\)\n\\(d = 0.3\\)\nan independent sample t-test situation\n\n\npower_analysis <- pwr::pwr.t.test(\n  d = 0.3, \n  power = 0.8,\n  sig.level = 0.05\n)\npower_analysis\n\n\n\n     Two-sample t test power calculation \n\n              n = 175.3847\n              d = 0.3\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\nFrom the output we need 175.3846666 subjects per group for reaching the desired power level. As said before a better approach is analyzing the entire power curve. We can simply plot the power_analysis object:\n\n\nplot(power_analysis)\n\n\n\n\nPower by simulation\nThe previous example is based on the analytically power computation that is possible for simple statistical test. A more general approach is the power analysis by simulation.\nIf we know the statistical assumptions of our analysis we can simulate data accordingly several times (e.g., 10000 simulations) and simply count the number of p values below the alpha level. This is a little bit too much for a simple t-test but can be really insightful.\nWe need to simulate two groups sampled from two populations with different mean (our effect size) and with the same standard deviation. We can simulate directly on the cohen's d scale setting the standard deviation to 1 and the mean difference to the desired d level.\nFor obtaining the power curve we need a range of sample size from 10 to 200 for example.\nThere are multiple ways to approach a simulation. Here I declare my parameters and create a grid of values using the tidyr::expand_grid() function to create all combinations. The I simply need to loop for each row, generate data using rnorm, calculate the t-test and then count how many p-values are below the alpha level.\n\n\nmp0 <- 0\nmp1 <- 0.3\nsd_p <- 1\n# d = mp1 - mp0 / sigma = (0.3 - 0) / 1 = 0.3\nsample_size <- seq(10, 200, 30)\nnsim <- 1000\nalpha_level <- 0.05\n\nsim <- expand_grid(\n  mp0,\n  mp1,\n  sd_p,\n  sample_size,\n  nsim = 1:nsim,\n  p_value = 0\n)\n\n# Using the for approach for clarity, *apply or map is better\n\nfor(i in 1:nrow(sim)){\n  g0 <- rnorm(sim$sample_size[i], sim$mp0[i], sim$sd_p[i])\n  g1 <- rnorm(sim$sample_size[i], sim$mp1[i], sim$sd_p[i])\n  sim[i, \"p_value\"] <- t.test(g0, g1)$p.value\n}\n\nsim %>% \n  group_by(sample_size, mp1) %>% \n  summarise(power = mean(p_value < alpha_level)) %>% \n  ggplot(aes(x = sample_size, y = power)) +\n  geom_point(size = 3) +\n  geom_line() +\n  ggtitle(paste(\"Effect size = \", sim$mp1[1]))\n\n\n\n\nThe result is very similar to the pwr result. Increasing the number of simulation will stabilize the results. As said before, using this approach for a t-test is not convenient but with the same code and idea we can simulate an unequal variance situation or having different sample size per group.\nSensitivity Analysis\nUsing the same approach as before, we can perform a sensivity analysis simply changing our free parameters in the previous simulation. The sensitivity analysis is usually performed with a given sample size and the the free parameter will be the effect size. We can use a range from 0 (the null effect) to 1 and fixing a sample size of 50 subjects per group.\n\n\nmp0 <- 0\nmp1 <- seq(0, 1, 0.2)\nsd_p <- 1\nsample_size <- 50\nnsim <- 1000\nalpha_level <- 0.05\n\nsim <- expand_grid(\n  mp0,\n  mp1,\n  sd_p,\n  sample_size,\n  nsim = 1:nsim,\n  p_value = 0\n)\n\n# Using the for approach for clarity, *apply or map is better\n\nfor(i in 1:nrow(sim)){\n  g0 <- rnorm(sim$sample_size[i], sim$mp0[i], sim$sd_p[i])\n  g1 <- rnorm(sim$sample_size[i], sim$mp1[i], sim$sd_p[i])\n  sim[i, \"p_value\"] <- t.test(g0, g1)$p.value\n}\n\nsim %>% \n  group_by(mp1, sample_size) %>% \n  summarise(power = mean(p_value < alpha_level)) %>% \n  ggplot(aes(x = mp1, y = power)) +\n  geom_point(size = 3) +\n  geom_line() +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", size = 1, col = \"red\") +\n  ggtitle(paste(\"Sample size = \", sim$sample_size[1]))\n\n\n\n\nWith the simulation approach we simply have to change our grid of values and calculate the power grouping for effect size instead of sample size. Here we understand that with a sample size of 50 we can detect with 80% power an effect size of ~0.6. If the true effect size is lower than the maximum detectable effect size, we are using an under-powered design.\nScript\n\n\n## -----------------------------------------------------------------------------\n## Script: Sensitivity analysis\n##\n## Author: Filippo Gambarota\n## -----------------------------------------------------------------------------\n\n# Packages ----------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(furrr)\n\n# Environment -------------------------------------------------------------\n\nset.seed(2021)\n\n# Functions ---------------------------------------------------------------\n\n# Find the closest target from a vector\n\nfind_closest_n <- function(vector, target){\n  index <- which.min(abs(vector - target))\n  out <- vector[index]\n  return(out)\n}\n\n# Return the minimun effect size given a sample size and the power level\n\nmin_effect <- function(data, sample_size, power_level){\n  ns <- find_closest_n(unique(data$sample_size), sample_size)\n  min(data$effect_size[data$sample_size == ns & data$power >= power_level])\n}\n\n# Calculate power\n\ncompute_power <- function(data, alpha){\n  data %>% \n    group_by(sample_size, effect_size) %>% \n    summarise(power = mean(ifelse(p_value < alpha, 1, 0)))\n}\n\n# Plot the contour\n\npower_contour <- function(data){\n  data %>% \n    ggplot(aes(x = sample_size, y = effect_size, z = power)) +\n    geom_contour_filled(breaks = seq(0,1,0.1)) +\n    coord_cartesian() +\n    cowplot::theme_minimal_grid()\n}\n\n# Plot the power curve\n\npower_curve <- function(data, n){\n  ns <- find_closest_n(unique(data$sample_size), n)\n  data %>% \n    filter(sample_size == ns) %>% \n    ggplot(aes(x = effect_size, y = power)) +\n    geom_point() +\n    geom_line() +\n    cowplot::theme_minimal_grid() +\n    ggtitle(paste(\"Sample size =\", ns))\n}\n\n# Setup simulation --------------------------------------------------------\n\nsample_size <- seq(10, 500, 50)\neffect_size <- seq(0, 1, 0.1)\nnsim <- 1000\n\nsim <- expand_grid(\n  sample_size,\n  effect_size,\n  sim = 1:nsim\n)\n\n# Test --------------------------------------------------------------------\n\nplan(multisession(workers = 4))\n\nsim$p_value <- furrr::future_map2_dbl(sim$sample_size, sim$effect_size, function(x, y){\n  g0 <- rnorm(x, 0, 1)\n  g1 <- rnorm(x, y, 1)\n  t.test(g0, g1)$p.value\n}, .options = furrr_options(seed = TRUE))\n\n# Computing power\n\nsim_power <- compute_power(sim, alpha = 0.05)\n\n# Plots -------------------------------------------------------------------\n\n# Contour plot\n\npower_contour(sim_power)\n\n# Power curve\n\npower_curve(sim_power, 200)\n\n\n\nBayesian\n\n\nsim <- expand_grid(\n  mp0,\n  mp1,\n  sd_p,\n  sample_size,\n  nsim = 1:nsim,\n  p_value = 0,\n  bf = 0\n)\n\n# Using the for approach for clarity, *apply or map is better\n\nfor(i in 1:nrow(sim)){\n  g0 <- rnorm(sim$sample_size[i], sim$mp0[i], sim$sd_p[i])\n  g1 <- rnorm(sim$sample_size[i], sim$mp1[i], sim$sd_p[i])\n  sim[i, \"p_value\"] <- t.test(g0, g1)$p.value\n  sim[i, \"bf\"] <- extractBF(ttestBF(g0,g1))$bf\n}\n\nsim %>% \n  group_by(mp1, sample_size) %>% \n  summarise(power = mean(p_value < alpha_level),\n            bf = mean(log(bf))) %>% \n  pivot_longer(c(power, bf), names_to = \"metric\", values_to = \".value\") %>% \n  ggplot(aes(x = mp1, y = .value)) +\n  facet_wrap(~metric, scales = \"free\") +\n  geom_point(size = 3) +\n  geom_line() +\n  ggtitle(paste(\"Sample size = \", sim$sample_size[1]))\n\n\n\n\nSession info\n\n\nsessionInfo()\n\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Pop!_OS 21.10\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3\nLAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] BayesFactor_0.9.12-4.2 Matrix_1.4-0          \n [3] coda_0.19-4            pwr_1.3-0             \n [5] forcats_0.5.1          stringr_1.4.0         \n [7] dplyr_1.0.7            purrr_0.3.4           \n [9] readr_2.1.1            tidyr_1.1.4           \n[11] tibble_3.1.6           ggplot2_3.3.5         \n[13] tidyverse_1.3.1       \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.2         sass_0.4.0         jsonlite_1.7.2    \n [4] modelr_0.1.8       gtools_3.9.2       bslib_0.3.1       \n [7] assertthat_0.2.1   highr_0.9          cellranger_1.1.0  \n[10] yaml_2.2.1         globals_0.14.0     pillar_1.6.4      \n[13] backports_1.4.1    lattice_0.20-45    glue_1.6.0        \n[16] digest_0.6.29      rvest_1.0.2        colorspace_2.0-2  \n[19] htmltools_0.5.2    pkgconfig_2.0.3    broom_0.7.10      \n[22] listenv_0.8.0      haven_2.4.3        mvtnorm_1.1-3     \n[25] scales_1.1.1       distill_1.3        tzdb_0.2.0        \n[28] downlit_0.4.0      MatrixModels_0.5-0 generics_0.1.1    \n[31] farver_2.1.0       ellipsis_0.3.2     furrr_0.2.3       \n[34] cachem_1.0.6       withr_2.4.3        pbapply_1.5-0     \n[37] cli_3.1.0          magrittr_2.0.1     crayon_1.4.2      \n[40] readxl_1.3.1       memoise_2.0.0      evaluate_0.14     \n[43] parallelly_1.30.0  future_1.23.0      fs_1.5.2          \n[46] fansi_0.5.0        xml2_1.3.3         tools_4.1.2       \n[49] hms_1.1.1          lifecycle_1.0.1    munsell_0.5.0     \n[52] reprex_2.0.1       compiler_4.1.2     jquerylib_0.1.4   \n[55] rlang_0.4.12       grid_4.1.2         rstudioapi_0.13   \n[58] labeling_0.4.2     rmarkdown_2.11     gtable_0.3.0      \n[61] codetools_0.2-18   DBI_1.1.2          R6_2.5.1          \n[64] lubridate_1.8.0    knitr_1.37         fastmap_1.1.0     \n[67] utf8_1.2.2         stringi_1.7.6      parallel_4.1.2    \n[70] Rcpp_1.0.7         vctrs_0.3.8        dbplyr_2.1.1      \n[73] tidyselect_1.1.1   xfun_0.29         \n\nReferences\n\n\n\n",
    "preview": "posts/sensitivity_analysis/distill-preview.png",
    "last_modified": "2022-01-05T01:24:49+01:00",
    "input_file": {}
  }
]
